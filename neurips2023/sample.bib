@article{gorbunov2022variance,
  title={Variance Reduction is an Antidote to Byzantines: Better Rates, Weaker Assumptions and Communication Compression as a Cherry on the Top},
  author={Gorbunov, Eduard and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter and Gidel, Gauthier},
  journal={ICLR},
  year={2023}
}

@article{beznosikov2022smooth,
  title={Smooth Monotone Stochastic Variational Inequalities and Saddle Point Problems--Survey},
  author={Beznosikov, Aleksandr and Polyak, Boris and Gorbunov, Eduard and Kovalev, Dmitry and Gasnikov, Alexander},
  journal={European Mathematical Society Magazine},
  year={2023}
}

@article{golowich2020tight,
  title={Tight last-iterate convergence rates for no-regret learning in multi-player games},
  author={Golowich, Noah and Pattathil, Sarath and Daskalakis, Constantinos},
  journal={NeurIPS},
  year={2020}
}

@article{necoara2019linear,
  title={Linear convergence of first order methods for non-strongly convex optimization},
  author={Nesterov, Yu},
  journal={Mathematical Programming},
  volume={175},
  pages={69--107},
  year={2019},
  publisher={Springer}
}

@article{combettes2004proximal,
  title={Proximal methods for cohypomonotone operators},
  author={Combettes, Patrick L and Pennanen, Teemu},
  journal={SIAM journal on control and optimization},
  volume={43},
  number={2},
  pages={731--742},
  year={2004},
  publisher={SIAM}
}

@article{gorbunov2022convergence,
  title={Convergence of Proximal Point and Extragradient-Based Methods Beyond Monotonicity: the Case of Negative Comonotonicity},
  author={Gorbunov, Eduard and Taylor, Adrien and Horv{\'a}th, Samuel and Gidel, Gauthier},
  journal={ICML},
  year={2023}
}

@inproceedings{gorbunov2022stochastic,
  title={Stochastic extragradient: General analysis and improved rates},
  author={Gorbunov, Eduard and Berard, Hugo and Gidel, Gauthier and Loizou, Nicolas},
  booktitle={AISTATS},
  year={2022}
}

@article{needell2013two,
  title={Two-subspace projection method for coherent overdetermined systems},
  author={Needell, Deanna and Ward, Rachel},
  journal={Journal of Fourier Analysis and Applications},
  volume={19},
  number={2},
  pages={256--269},
  year={2013},
  publisher={Springer}
}

@article{beznosikov2022stochastic,
  title={Stochastic gradient descent-ascent: Unified theory and new efficient methods},
  author={Beznosikov, Aleksandr and Gorbunov, Eduard and Berard, Hugo and Loizou, Nicolas},
  journal={AISTATS},
  year={2023}
}

@article{lee2021fast,
  title={Fast extra gradient methods for smooth structured nonconvex-nonconcave minimax problems},
  author={Lee, Sucheol and Kim, Donghwan},
  journal={NeurIPS},
  year={2021}
}

@article{mertikopoulos2019learning,
  title={Learning in games with continuous action sets and unknown payoff functions},
  author={Mertikopoulos, Panayotis and Zhou, Zhengyuan},
  journal={Mathematical Programming},
  volume={173},
  number={1},
  pages={465--507},
  year={2019},
  publisher={Springer}
}

@inproceedings{daskalakis2021complexity,
  title={The complexity of constrained min-max optimization},
  author={Daskalakis, Constantinos and Skoulakis, Stratis and Zampetakis, Manolis},
  booktitle={Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing},
  pages={1466--1478},
  year={2021}
}

@inproceedings{song2020optimistic,
    author = {Song, Chaobing and Zhou, Zhengyuan and Zhou, Yichao and Jiang, Yong and Ma, Yi},
    booktitle = {NeurIPS},
    title = {Optimistic Dual Extrapolation for Coherent Non-monotone Variational Inequalities},
    year = {2020}
}

@inproceedings{diakonikolas2021efficient,
  title={Efficient methods for structured nonconvex-nonconcave min-max optimization},
  author={Diakonikolas, Jelena and Daskalakis, Constantinos and Jordan, Michael},
  booktitle={AISTATS},
  year={2021}
}


@article{facchinei2007games,
	title={Generalized {N}ash equilibrium problems},
	author={Facchinei, Francisco and Kanzow, Christian},
	journal={4OR},
	volume={5},
	number={3},
	pages={173--210},
	year={2007},
}

@inproceedings{goodfellow2014generative,
	title={Generative adversarial nets},
	author={Goodfellow, I. and Pouget-Abadie, J. and Mirza, M. and Xu, B. and Warde-Farley, D. and Ozair, S. and Courville, A. and Bengio, Y.},
	booktitle={NeurIPS},
	year={2014}
}



@inproceedings{yu2022fast,
  title={Fast Distributionally Robust Learning with Variance-Reduced Min-Max Optimization},
  author={Yu, Yaodong and Lin, Tianyi and Mazumdar, Eric V and Jordan, Michael},
  booktitle={AISTATS},
  year={2022}
}

@article{namkoong2016stochastic,
  title={Stochastic gradient methods for distributionally robust optimization with f-divergences},
  author={Namkoong, Hongseok and Duchi, John C},
  journal={NeurIPS},
  year={2016}
}


@article{brown2020combining,
  title={Combining deep reinforcement learning and search for imperfect-information games},
  author={Brown, Noam and Bakhtin, Anton and Lerer, Adam and Gong, Qucheng},
  journal={NeurIPS},
  year={2020}
}


@article{wang2021adversarial,
  title={Adversarial attack generation empowered by min-max optimization},
  author={Wang, Jingkang and Zhang, Tianyun and Liu, Sijia and Chen, Pin-Yu and Xu, Jiacen and Fardad, Makan and Li, Bo},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{madry2018towards,
  title={Towards Deep Learning Models Resistant to Adversarial Attacks},
  author={Madry, A. and Makelov, A. and Schmidt, L. and Tsipras, D. and Vladu, A.},
  booktitle={ICLR},
  year={2018}
}


@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={ICML},
  year={2017}
}

@article{loizou2020convergence,
  title={Convergence analysis of inexact randomized iterative methods},
  author={Loizou, Nicolas and Richt{\'a}rik, Peter},
  journal={SIAM Journal on Scientific Computing},
  volume={42},
  number={6},
  pages={A3979--A4016},
  year={2020},
  publisher={SIAM}
}

@article{loizou2020momentum,
  title={Momentum and stochastic momentum for stochastic gradient, newton, proximal point and subspace descent methods},
  author={Loizou, Nicolas and Richt{\'a}rik, Peter},
  journal={Computational Optimization and Applications},
  volume={77},
  number={3},
  pages={653--710},
  year={2020},
  publisher={Springer}
}

@inproceedings{beznosikov2023compression,
  title={Compression and data similarity: Combination of two techniques for communication-efficient solving of distributed variational inequalities},
  author={Beznosikov, Aleksandr and Gasnikov, Alexander},
  booktitle={Optimization and Applications: 13th International Conference, OPTIMA 2022, Petrovac, Montenegro, September 26--30, 2022, Revised Selected Papers},
  pages={151--162},
  year={2023},
  organization={Springer}
}

@article{hsieh2019convergence,
  title={On the convergence of single-call stochastic extra-gradient methods},
  author={Hsieh, Yu-Guan and Iutzeler, Franck and Malick, J{\'e}r{\^o}me and Mertikopoulos, Panayotis},
  journal={NeurIPS},
  year={2019}
}

@article{gower2018accelerated,
  title={Accelerated stochastic matrix inversion: general theory and speeding up BFGS rules for faster second-order optimization},
  author={Gower, Robert and Hanzely, Filip and Richt{\'a}rik, Peter and Stich, Sebastian U},
  journal={NeurIPS},
  year={2018}
}

@article{richtarik2020stochastic,
  title={Stochastic reformulations of linear systems: algorithms and convergence theory},
  author={Richt{\'a}rik, Peter and Tak{\'a}c, Martin},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={41},
  number={2},
  pages={487--524},
  year={2020},
  publisher={SIAM}
}

@article{hsieh2020explore,
  title={Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling},
  author={Hsieh, Yu-Guan and Iutzeler, Franck and Malick, J{\'e}r{\^o}me and Mertikopoulos, Panayotis},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{mishchenko2020revisiting,
  title={Revisiting stochastic extragradient},
  author={Mishchenko, Konstantin and Kovalev, Dmitry and Shulgin, Egor and Richt{\'a}rik, Peter and Malitsky, Yura},
  booktitle={AISTATS},
  year={2020}
}

@article{stich2019unified,
  title={Unified optimal analysis of the (stochastic) gradient method},
  author={Stich, Sebastian U},
  journal={arXiv preprint arXiv:1907.04232},
  year={2019}
}

@article{bohm2022solving,
  title={Solving Nonconvex-Nonconcave Min-Max Problems exhibiting Weak Minty Solutions},
  author={B{\"o}hm, Axel},
  journal={arXiv preprint arXiv:2201.12247},
  year={2022}
}


@article{gidel2018variational,
  title={A variational inequality perspective on generative adversarial networks},
  author={Gidel, Gauthier and Berard, Hugo and Vignoud, Ga{\"e}tan and Vincent, Pascal and Lacoste-Julien, Simon},
  journal={ICLR},
  year={2019}
}

@article{ryu2019ode,
  title={Ode analysis of stochastic gradient methods with optimism and anchoring for minimax problems},
  author={Ryu, Ernest K and Yuan, Kun and Yin, Wotao},
  journal={arXiv preprint arXiv:1905.10899},
  year={2019}
}

@inproceedings{mokhtari2020unified,
  title={A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach},
  author={Mokhtari, Aryan and Ozdaglar, Asuman and Pattathil, Sarath},
  booktitle={AISTATS},
  year={2020}
}

@article{gorbunov2022last,
  title={Last-Iterate Convergence of Optimistic Gradient Method for Monotone Variational Inequalities},
  author={Gorbunov, Eduard and Taylor, Adrien and Gidel, Gauthier},
  journal={arXiv preprint arXiv:2205.08446},
  year={2022}
}

@inproceedings{gower2019sgd,
  title={SGD: General analysis and improved rates},
  author={Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
  booktitle={ICML},
  year={2019}
}

@article{noor2003new,
  title={New extragradient-type methods for general variational inequalities},
  author={Noor, Muhammad Aslam},
  journal={Journal of Mathematical Analysis and Applications},
  volume={277},
  number={2},
  pages={379--394},
  year={2003},
  publisher={Elsevier}
}

@article{loizou2021stochastic,
  title={Stochastic gradient descent-ascent and consensus optimization for smooth games: Convergence analysis under expected co-coercivity},
  author={Loizou, Nicolas and Berard, Hugo and Gidel, Gauthier and Mitliagkas, Ioannis and Lacoste-Julien, Simon},
  journal={NeurIPS},
  year={2021}
}

@article{korpelevich1976extragradient,
  title={The extragradient method for finding saddle points and other problems},
  author={Korpelevich, Galina M},
  journal={Matecon},
  volume={12},
  pages={747--756},
  year={1976}
}

@article{popov1980modification,
  title={A modification of the Arrow-Hurwicz method for search of saddle points},
  author={Popov, Leonid Denisovich},
  journal={Mathematical notes of the Academy of Sciences of the USSR},
  volume={28},
  number={5},
  pages={845--848},
  year={1980},
  publisher={Springer}
}

@article{cai2022tight,
  title={Tight last-iterate convergence of the extragradient method for constrained monotone variational inequalities},
  author={Cai, Yang and Oikonomou, Argyris and Zheng, Weiqiang},
  journal={arXiv preprint arXiv:2204.09228},
  year={2022}
}

@inproceedings{li2022convergence,
  title={On the convergence of stochastic extragradient for bilinear games using restarted iteration averaging},
  author={Li, Chris Junchi and Yu, Yaodong and Loizou, Nicolas and Gidel, Gauthier and Ma, Yi and Le Roux, Nicolas and Jordan, Michael},
  booktitle={AISTATS},
  year={2022}
}

@article{d2021stochastic,
  title={On Stochastic Mirror Descent: Convergence Analysis and Adaptive Variants},
  author={D'Orazio, Ryan and Loizou, Nicolas and Laradji, Issam and Mitliagkas, Ioannis},
  journal={ICML},
  year={2021}
}

@article{vaswani2019painless,
  title={Painless stochastic gradient: Interpolation, line-search, and convergence rates},
  author={Vaswani, Sharan and Mishkin, Aaron and Laradji, Issam and Schmidt, Mark and Gidel, Gauthier and Lacoste-Julien, Simon},
  journal={NeurIPS},
  year={2019}
}

@inproceedings{loizou2016new,
  title={A new perspective on randomized gossip algorithms},
  author={Loizou, Nicolas and Richt{\'a}rik, Peter},
  booktitle={2016 IEEE global conference on signal and information processing (GlobalSIP)},
  pages={440--444},
  year={2016},
  organization={IEEE}
}

@article{loizou2021revisiting,
  title={Revisiting randomized gossip algorithms: General framework, convergence rates and novel block and accelerated protocols},
  author={Loizou, Nicolas and Richt{\'a}rik, Peter},
  journal={IEEE Transactions on Information Theory},
  volume={67},
  number={12},
  pages={8300--8324},
  year={2021},
  publisher={IEEE}
}

@article{khaled2020unified,
  title={Unified analysis of stochastic gradient methods for composite convex and smooth optimization},
  author={Khaled, Ahmed and Sebbouh, Othmane and Loizou, Nicolas and Gower, Robert M and Richt{\'a}rik, Peter},
  journal={Journal of Optimization Theory and Applications},
  year={2020}
}

@article{khaled2020better,
  title={Better theory for SGD in the nonconvex world},
  author={Khaled, Ahmed and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2002.03329},
  year={2020}
}

@inproceedings{gower2021sgd,
  title={Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation},
  author={Gower, Robert and Sebbouh, Othmane and Loizou, Nicolas},
  booktitle={AISTATS},
  year={2021}
}

@article{richtarik2013optimal,
  title={On optimal probabilities in stochastic coordinate descent methods},
  author={Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
  journal={arXiv preprint arXiv:1310.3438},
  year={2013}
}

@inproceedings{hanzely2019accelerated,
  title={Accelerated coordinate descent with arbitrary sampling and best rates for minibatches},
  author={Hanzely, Filip and Richt{\'a}rik, Peter},
  booktitle={AISTATS},
  year={2019}
}

@article{qu2016coordinate,
  title={Coordinate descent with arbitrary sampling I: Algorithms and complexity},
  author={Qu, Zheng and Richt{\'a}rik, Peter},
  journal={Optimization Methods and Software},
  volume={31},
  number={5},
  pages={829--857},
  year={2016},
  publisher={Taylor \& Francis}
}


@inproceedings{horvath2019nonconvex,
  title={Nonconvex variance reduced optimization with arbitrary sampling},
  author={Horv{\'a}th, Samuel and Richt{\'a}rik, Peter},
  booktitle={ICML},
  year={2019}
}

@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}

@inproceedings{abernethy2021last,
  title={Last-iterate convergence rates for min-max optimization: Convergence of hamiltonian gradient descent and consensus optimization},
  author={Abernethy, Jacob and Lai, Kevin A and Wibisono, Andre},
  booktitle={Algorithmic Learning Theory},
  pages={3--47},
  year={2021},
  organization={PMLR}
}

@article{juditsky2011solving,
  title={Solving variational inequalities with stochastic mirror-prox algorithm},
  author={Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire},
  journal={Stochastic Systems},
  volume={1},
  number={1},
  pages={17--58},
  year={2011},
  publisher={INFORMS}
}

@article{yang2020global,
  title={Global convergence and variance-reduced optimization for a class of nonconvex-nonconcave minimax problems},
  author={Yang, Junchi and Kiyavash, Negar and He, Niao},
  journal={NeurIPS},
  year={2020}
}

@article{tran2020hybrid,
  title={Hybrid variance-reduced SGD algorithms for minimax problems with nonconvex-linear function},
  author={Tran Dinh, Quoc and Liu, Deyi and Nguyen, Lam},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{lin2020gradient,
  title={On gradient descent ascent for nonconvex-concave minimax problems},
  author={Lin, Tianyi and Jin, Chi and Jordan, Michael},
  booktitle={ICML},
  year={2020}
}

@inproceedings{lin2020finite,
  title={Finite-time last-iterate convergence for multi-agent learning in games},
  author={Lin, Tianyi and Zhou, Zhengyuan and Mertikopoulos, Panayotis and Jordan, Michael},
  booktitle={ICML},
  year={2020}
}

@article{sokota2022unified,
  title={A unified approach to reinforcement learning, quantal response equilibria, and two-player zero-sum games},
  author={Sokota, Samuel and D'Orazio, Ryan and Kolter, J Zico and Loizou, Nicolas and Lanctot, Marc and Mitliagkas, Ioannis and Brown, Noam and Kroer, Christian},
  journal={ICLR},
  year={2023}
}


@article{chavdarova2019reducing,
  title={Reducing noise in gan training with variance reduced extragradient},
  author={Chavdarova, Tatjana and Gidel, Gauthier and Fleuret, Fran{\c{c}}ois and Lacoste-Julien, Simon},
  journal={NeurIPS},
  year={2019}
}

@article{daskalakis2020independent,
  title={Independent policy gradient methods for competitive reinforcement learning},
  author={Daskalakis, Constantinos and Foster, Dylan J and Golowich, Noah},
  journal={NeurIPS},
  year={2020}
}

@article{bauschke2021generalized,
  title={Generalized monotone operators and their averaged resolvents},
  author={Bauschke, Heinz H and Moursi, Walaa M and Wang, Xianfu},
  journal={Mathematical Programming},
  volume={189},
  number={1},
  pages={55--74},
  year={2021},
  publisher={Springer}
}


@article{liu2016accelerated,
  title={An accelerated randomized Kaczmarz algorithm},
  author={Liu, Ji and Wright, Stephen},
  journal={Mathematics of Computation},
  volume={85},
  number={297},
  pages={153--178},
  year={2016}
}

@article{pfau2016connecting,
  title={Connecting generative adversarial networks and actor-critic methods},
  author={Pfau, David and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1610.01945},
  year={2016}
}

@article{wayne2014hierarchical,
  title={Hierarchical control using networks trained with higher-level forward models},
  author={Wayne, Greg and Abbott, LF},
  journal={Neural computation},
  volume={26},
  number={10},
  pages={2163--2193},
  year={2014},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{dikkala2020minimax,
  title={Minimax estimation of conditional moment models},
  author={Dikkala, Nishanth and Lewis, Greg and Mackey, Lester and Syrgkanis, Vasilis},
  journal={NeurIPS},
  year={2020}
}

@article{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  journal={NeurIPS},
  year={2019}
}

@article{rolinek2018l4,
  title={L4: Practical loss-based stepsize adaptation for deep learning},
  author={Rolinek, Michal and Martius, Georg},
  journal={NeurIPS},
  year={2018}
}

@inproceedings{vaswani2019fast,
  title={Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron},
  author={Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
  booktitle={AISTATS},
  year={2019}
}


@InProceedings{pmlr-v130-loizou21a,
  title = 	 { Stochastic Polyak Step-size for SGD: An Adaptive Learning Rate for Fast Convergence },
  author = {Loizou, Nicolas and Vaswani, Sharan and Hadj Laradji, Issam and Lacoste-Julien, Simon},
  booktitle = {AISTATS},
  year = 	 {2021},  
}

@article{daskalakis2017training,
  title={Training gans with optimism},
  author={Daskalakis, Constantinos and Ilyas, Andrew and Syrgkanis, Vasilis and Zeng, Haoyang},
  journal={arXiv preprint arXiv:1711.00141},
  year={2017}
}

@article{dang2015convergence,
  title={On the convergence properties of non-euclidean extragradient methods for variational inequalities with generalized monotone operators},
  author={Dang, Cong D and Lan, Guanghui},
  journal={Computational Optimization and applications},
  volume={60},
  number={2},
  pages={277--310},
  year={2015},
  publisher={Springer}
}

@article{liu2019towards,
  title={Towards better understanding of adaptive gradient algorithms in generative adversarial nets},
  author={Liu, Mingrui and Mroueh, Youssef and Ross, Jerret and Zhang, Wei and Cui, Xiaodong and Das, Payel and Yang, Tianbao},
  journal={ICLR},
  year={2020}
}

@article{liu2021first,
  title={First-order Convergence Theory for Weakly-Convex-Weakly-Concave Min-max Problems.},
  author={Liu, Mingrui and Rafique, Hassan and Lin, Qihang and Yang, Tianbao},
  journal={J. Mach. Learn. Res.},
  volume={22},
  pages={169--1},
  year={2021}
}

@article{mertikopoulos2018optimistic,
  title={Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile},
  author={Mertikopoulos, Panayotis and Lecouat, Bruno and Zenati, Houssam and Foo, Chuan-Sheng and Chandrasekhar, Vijay and Piliouras, Georgios},
  journal={ICLR},
  year={2019}
}

@inproceedings{pethick2022escaping,
  title={Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems},
  author={Pethick, Thomas and Patrinos, Panagiotis and Fercoq, Olivier and Cevher{\aa}, Volkan and others},
  booktitle={ICLR},
  year={2022}
}

@article{sebbouh2019towards,
  title={Towards closing the gap between the theory and practice of SVRG},
  author={Sebbouh, Othmane and Gazagnadou, Nidham and Jelassi, Samy and Bach, Francis and Gower, Robert},
  journal={NeurIPS},
  year={2019}
}

@article{dem1972numerical,
  title={Numerical methods for finding saddle points},
  author={Dem'yanov, Vladimir Fedorovich and Pevnyi, Aleksandr Borisovich},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={12},
  number={5},
  pages={11--52},
  year={1972},
  publisher={Elsevier}
}

@article{szlendak2021permutation,
  title={Permutation compressors for provably faster distributed nonconvex optimization},
  author={Szlendak, Rafa{\l} and Tyurin, Alexander and Richt{\'a}rik, Peter},
  journal={ICLR},
  year={2022}
}

@inproceedings{qian2019saga,
  title={SAGA with arbitrary sampling},
  author={Qian, Xun and Qu, Zheng and Richt{\'a}rik, Peter},
  booktitle={ICML},
  year={2019}
}

@inproceedings{loizou2020stochastic,
  title={Stochastic hamiltonian gradient methods for smooth games},
  author={Loizou, Nicolas and Berard, Hugo and Jolicoeur-Martineau, Alexia and Vincent, Pascal and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
  booktitle={ICML},
  year={2020}
}

@article{solodov1999hybrid,
  title={A hybrid approximate extragradient--proximal point algorithm using the enlargement of a maximal monotone operator},
  author={Solodov, Mikhail V and Svaiter, Benar F},
  journal={Set-Valued Analysis},
  volume={7},
  number={4},
  pages={323--345},
  year={1999},
  publisher={Springer}
}

@inproceedings{gorbunov2022extragradient,
  title={Extragradient method: O (1/K) last-iterate convergence for monotone variational inequalities and connections with cocoercivity},
  author={Gorbunov, Eduard and Loizou, Nicolas and Gidel, Gauthier},
  booktitle={AISTATS},
  year={2022}
}

@article{pethick2023solving,
  title={Solving stochastic weak Minty variational inequalities without increasing batch size},
  author={Pethick, Thomas and Fercoq, Olivier and Latafat, Puya and Patrinos, Panagiotis and Cevher, Volkan},
  journal={ICLR},
  year={2023}
}
